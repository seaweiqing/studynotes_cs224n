# week 1: word2vec, skip-gram
<p style=""></p><p><span style="font-size: small;">- lecture 1: class introduction</span></p><p><font size="2">- lecture 2: word2vec</font></p><p><font size="2">- word2vec tutorial: skip gram neural network architecture for Word2Vec</font></p><p><font size="2">- lecture notes01: Natural Language Processing. Word Vectors. Singular Value Decomposition. Skip-gram. Continuous Bag of Words (CBOW). Negative Sampling. Hierarchical Softmax. Word2Vec</font></p><p><font size="2">- paper: Distributed Representations ofWords and Phrases and their Compositionality: subsampling, negative sampling, method for finding phrases</font></p><p><font size="2">- paper: Efficient Estimation of Word Representations in Vector Space: dealing with large data set, achieving large improvements in accuracy at much lower computational cost</font></p><div><br></div><p></p>
